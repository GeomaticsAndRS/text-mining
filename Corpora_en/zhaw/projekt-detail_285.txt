In this project should be clarified, how far today and in the near future an automatic implementation of Visual communication by sign language (GS) in text can be reached available technical AIDS. Goal is to enable a hearing impaired without an interpreter to take part in a meeting, with a hearing person calls or to communicate, or to learn the GS and to train. The goal of this project is to create a platform for the development of a robust multimodal GS communication platform based on various sensor signals. A first functional prototype for the automatic implementation of GS in text should be implemented and demonstrated as proof of concept (PoC) of the platform. It examines one of the two following variants: 1 detecting the GS have the body position (arms, hands and the further detail also finger and facial expressions) be captured. The individual gestures are implemented in words, making rudimentary sentences strung together (for example: you I give Apple). It is important to be able to recognize the basic thrust of the gestures. The grammatically correct formation of the rates is not yet in the foreground. 2. to be able to recognize the finger alphabet (FA), you must identify the positions of each finger. The recognized letters are strung together words, which are in turn whole sentences. So its text corresponds to what expresses the hard of hearing 1:1. For reliable detection of the FA, the position of all fingers of one hand is needed, no matter whether curved or straight. To the 3D of the finger positions will be evaluated (X, Y, Z (depth =)).
