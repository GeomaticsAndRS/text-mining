Exploiting Semantic Information for Image-Based Localization The main goal of this project is to investigate how semantic information about the content of an image can be used to improve image-based localization approaches that aim to determine the position and orientation from which the image was taken. Image-based localization methods aim to determine the position and orientation from which an test image was taken. A popular approach is to establish correspondences between 2D image positions and 3D point positions in some 3D model of the scene, which is all information that is required to solve for pose, i.e., the orientation and position, of the image. The 3D models of the scene are often obtained from Structure-from-Motion, i.e., by reconstructing the scene from a set of images. This enables us to establish the 2D-3D correspondences required for pose estimation by solving a descriptor matching problem, matching the descriptors associated with features found in the test image against the image descriptors associated with the 3D points. Classical image-based localization methods rely purely on the expressive power of such local features to establish the 2D-3D correspondences. In this project, we investigate if and how we can exploit semantic information about the content of the test image to improve the matching stage.
