SNF 2000211 57101/13DImageU nderstandingf orU rbanScenes The topic of the proposed project is automatic generation of 3D city models, starting from aerial and terrestrial image data. The topic of the proposed project is automatic generation of virtual 3D city models, starting from aerial and terrestrial image data. Digital models of our environment form the basis for geographic information systems (GIS) and are required for a wide range of tasks in planning, construction, navigation, etc. Such topographic data has been used for a long time by specialists and authorities, and is nowadays becoming ubiquitous with internet cartography. To this day, 3D city models are generated manually or at most semi-automatically, which is costly and does not scale. The main data source for large-scale mapping are images and range images acquired with cameras and laser scanners, either from aerial platforms or from mobile mapping systems on the ground. To convert that sensor data to a virtual 3D model one needs to solve two vision problems: on the one hand the 3D geometry of the imaged scene must be reconstructed, on the other hand the data must be interpreted, meaning that it must be structured into semantically meaningful entities (buildings, roads, trees, etc.). A crucial point that has long been known, but is still not properly accounted for, is that the two tasks are not independent - objects of different semantic classes have different geometric properties, and vice versa the geometric shape and extent is an important cue about the semantic object class of a surface. The aim of the proposed project is to develop computer vision techniques able to jointly solve 3D reconstruction and semantic labelling, to automatically generate complete and accurate, interpreted 3D models. Specifically, the project aims to develop a probabilistic framework for integrated 3D image understanding in a city modelling context, using aerial nadir, aerial oblique, and groundlevel images. Inference in that model shall deliver at the same time better 3D geometry, by allowing one to include category-specific priors for the surfacesS shape, orientation and layout; and better segmentation of the scene into semantic object classes, aided by the underlying 3D shape and layout. We are confident that integrated 3D modelling and scene understanding will significantly enhance the power of automatic mapping and constitutes an important step towards the visionary S3D ¸ vir tual habitatT.
