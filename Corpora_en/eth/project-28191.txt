Submodular Optimization in Machine Learning Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions in many domains. We are studying submodularity and its implications for challenging sparse representation, inference, learning and decision problems. Convex optimization has become a main workhorse for many machine learning algorithms during the past ten years. When minimizing a convex loss function for, e.g., training a Support Vector Machine, we can rest assured to efficiently find an optimal solution, even for large problems. In recent years, another fundamental problem structure, which has similar beneficial properties, has emerged as very useful in a variety of machine learning applications: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions. We are studying submodularity and its implications for challenging sparse representation, inference and learning problems.
