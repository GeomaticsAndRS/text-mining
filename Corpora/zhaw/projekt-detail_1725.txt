The main result of this project is a concept for generating a multimodal user interface  (MMUI) for a target application  from an abstract description of the tasks and concepts in this application: The generation process starts with a description of the tasks and concepts of the application in an OOA-Model augmented with additional information for the different modalities (GUI, speech input/output). In a first step this description is transformed into an abstract description of the whole MMUI comprising different screens. Each screen is build from a hierarchy of so-called multimodal objects (MMO). An MMO is a new concept of a multimodal component, we have developed. In a second step the actual MMUI for a specific device is generated from the abstract description of the MMUI. In this step, the MMUI-designer can fine tune the whole MMUI interaction. The result of this step is either a running MMUI, which finally must be connected with the application logic. Another possibility is to generate a standardized XML-description of the actual MMUI for later integration.