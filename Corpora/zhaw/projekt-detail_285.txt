In diesem Projekt soll abgeklärt werden, wie weit mit heute und in absehbarer Zeit verfügbaren technischen Hilfsmitteln eine automatische Umsetzung der visuellen Kommunikation von Gebärdensprache (GS) in Text erreicht werden kann.  
Ziel dabei ist es, einem Hörbehinderten zu ermöglichen, ohne einen Dolmetscher an einem Meeting teilzunehmen, mit einer hörenden Person zu telefonieren oder zu kommunizieren, oder die GS zu erlernen und zu trainieren.  
Das Ziel dieses Projektes ist die Schaffung einer Plattform für die Entwicklung einer robusten multimodalen GS-Kommunikationsplattform basierend auf verschiedenen Sensor-Signalen. Als Proof of Concept (PoC) der Plattform soll ein erster funktionaler Prototyp für die automatische Umsetzung von GS in Text umgesetzt und demonstriert werden. Es wird eine der zwei folgenden Varianten untersucht:  
1. Zur Erkennung der GS muss die Körperstellung (Arme, Hände und zur weiteren Detailierung auch Finger und Gesichtsmimik) erfasst werden. Die einzelnen Gesten werden in Wörter umgesetzt, welche aneinander gereiht rudimentäre Sätze bilden (z.B.: Du ich Apfel geben). Wichtig ist es, die Grundaussage der Gesten erkennen zu können. Die grammatikalisch korrekte Bildung der Sätze steht dabei noch nicht im Vordergrund.  
2. Um das Fingeralphabet (FA) erkennen zu können, müssen die Stellungen der einzelnen Finger identifiziert werden. Die erkannten Buchstaben ergeben aneinander gereiht Wörter, welche wiederum ganze Sätze bilden. Der so kommunizierte Text entspricht 1:1 dem, was der Hörbehinderte ausdrückt. Für eine sichere Erkennung des FA wird die Stellung von allen Fingern einer Hand benötigt, egal ob gekrümmt oder gestreckt. Dazu wird die 3D-Information der Fingerpositionen ausgewertet ( X, Y, Z (=Tiefe)).