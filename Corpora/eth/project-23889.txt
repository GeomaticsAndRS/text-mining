  Iterative Learning  
Humans are able to acquire incredible skills by practicing particular tasks over and over again. Our goal is to equip dynamic systems with the ability to learn from past experience and thus perform amazingly agile, adapted and accurate motions. 
Why not allowing autonomous robots to practice before demonstrating their artistic skills? And why arenSt automated systems able to improve their performance when repeatedly executing the same task? Our goal is to develop algorithms that enable autonomous systems to learn through practice. By making the information about previous trials available to the system, learning rules are designed which exploit the repetitiveness of the execution. Existing knowledge about the system dynamics, e.g. coarse first principle models, is used to derive these learning rules. Initial learning algorithms fall in the area of iterative learning control. Here, traditional optimal filtering methods are combined with state-of-the- art convex optimization techniques in order to first estimate the error between the desired motion the actual outcome and then to correct for it. This yields a more appropriate open-loop input that is applied in the next trial. Importantly, the derived formalism allows for the direct treatment of input and state constraints. After a first successful implementation on a cart-pendulum system that demonstrates the effectiveness of the learning scheme, the question arose: If having multiple similar vehicles - imagine a fleet of robots - is it possible to benefit from exchanging information during the learning process? Recently, some very first theoretic results were derived.