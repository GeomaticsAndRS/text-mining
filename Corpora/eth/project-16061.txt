  Oracle inequalities with the LASSO  
The LASSO is a regularization technique that does embedded feature selection. In this project, we show that the prediction error of the LASSO is up to constants as good as the one of the optimal model which discards irrelevant variables. 
Consider a model with more variables, or features, then there are observations. Suppose we know that the model contains many irrelevant variables, but we do not know which ones. The LASSO is a regularization technique that does embedded feature selection. In this project, we show that the prediction error of the LASSO is up to constants as good as the one of the optimal model which discards irrelevant variables. Moreover, we show that the oracle is mimicked for a a universal choice for the smoothing parameter. The result is stated in a non-asymptotic framework, giving acceptable bounds for finite sample sizes. Finally, our results do not rely on artificial technical conditions. The work extends earlier work, where only particular loss functions were studied.