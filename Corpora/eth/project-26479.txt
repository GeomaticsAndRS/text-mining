  Automatic Visual Learning through Multi-modal Interaction  
People effortlessly recognize thousands of object categories, such as cars, buildings, and horses. Achieving this with an artificial system is a major goal of computer vision, but the state-of-art is still far from human-level performance.
People effortlessly recognize thousands of object categories, such as cars, buildings, and horses. Achieving this with an artificial system is a major goal of computer vision, but the state-of-art is still far from human-level performance. The difficulty in learning and recognizing objects stems from the great appearance variations induced by changing camera positions and scene illuminations, partial occlusion, and variations within a category (e.g. cars of different brands). Most systems require substantial amounts of supervision to learn a category, varying from labeling images by the categories they contain, to precisely marking object outlines. Manual annotation is very time consuming and a major obstacle to the aim of learning thousands of categories. Although a few works try learning without any supervision, the lower recognition performance and high computational complexity raise doubts that these will scale up to a large number of categories. Goal --In this project we research visual learning within an alternative scenario, where the amount of manual annotation is kept very low by exploiting a widely available, virtually infinite source of images and accompanying annotations: the internet. In this scenario, a category is learnt from 1) a small amount of data supplied by the user (e.g. a single image labeled by a word), plus 2) a large amount harvested from the internet (e.g. by applying image search engines for the same word). These two sources of data are not equally reliable. While the user provides correct annotations, search engines return many images unrelated to the query word, because they rely on filenames and surrounding texts, disregarding actual image content. In addition to search engines, sites such as Flickr, online newspapers, and online shops host images accompanied by textual descriptions. These constitute unreliable annotations, as only some words correspond to scene elements appearing in the image. The goal of this research is to develop visual learning algorithms tailored to the complementary characteristics of these two data sources (the user and the internet) and to leverage the process by combining data modalities (images and text). Thanks to this, it will become possible to learn high quality visual models for a large number of object categories with minimal user intervention. In addition to object categories, this project covers another two visual entities in the same framework: object attributes, such as `striped', and human actions, such as `climbing'. Their counterparts in the text domain are adjectives and verbs, just like object categories correspond to nouns. Actions are best studied in video data, for which rich sources of textual information exist, such as subtitles and scripts. These are available on the internet for most TV series and movies. As a final research strand, we want to explore the joint treatment of the three word types by exploiting the structure of entire sentences. By drawing a parallel between words forming a sentence and their visual counterparts forming a scene, text will become a valuable aid for visual learning and a powerful guide for visual search. The mutual context the different visual entities provide each other will greatly reduce annotation ambiguity. Significance -------- The combined use of images and text will improve web search engines themselves. The joint analysis of objects, attributes, and actions will bring Computer Vision forward towards genuine scene understanding. This will make possible to organize movies and TV programs by content, it will improve the performance of surveillance and accident warning systems in cars, and it will be crucial for service robots for the elderly. Learning and recognizing objects and actions is a fundamental ability of human intelligence, which underpins much of our understanding of the world. The main c ETH Zürich, all rights reserved. Last updated: 06.08.2004 <page 2> contribution of this research is to substantially ease the task of teaching this ability to machines.