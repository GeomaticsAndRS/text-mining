  Location-Aware Mobile Eye Tracking for Tourist Assistance 
The project will pioneer gaze-based interaction techniques for tourists in outdoor environments. It envisions mobile assistance systems that trigger information services based on the user's gaze.
Mobile technologies have altered the way touristic information is presented and interacted with: analog guide books have been replaced by smartphones, which in turn are likely to be succeeded by emerging mobile technologies, such as augmented reality glasses (refer to the Google Project Glass). While these devices have the advantage of being less distractive, not requiring the user to change the visual attention to a handheld screen, it is not yet clear how the user input can be designed in an intuitive, efficient, and privacy-preserving way. Gaze-based interaction offers an efficient and privacy-preserving input modality, especially in usage scenarios where manual and voice interaction are limited. By processing the user's gaze position in real-time we can design intelligent assistance without distracting the user's visual attention. While most gaze-based interaction approaches so far remain limited to indoor environments and desktop or mobile screens, new generations of mobile eye tracking hardware allow for outdoor usage. The proposed project will pioneer gaze-based interaction techniques for tourists in outdoor environments. The project envisions mobile assistance systems that trigger information services based on the user's gaze on touristic areas of interest. For instance, a gaze-based recommender system could notify the observer of a city panorama about buildings that match her interest, given the objects she has looked at before. The main objective of this project consists in the investigation of novel gaze-based interaction methods for tourists exploring a city panorama. Questions to be answered include: how can we detect interest for touristic objects from a person's gaze? How can we notify the tourist of "gazable" objects in the environment, i.e., those that allow for interaction? The project will employ eye-tracking studies with tourists visually exploring a city panorama. Multimodal approaches combining gaze, audio, and vibration will be considered. Machine learning methods will be used to recognize touristic interest from gaze. Using a "triangulated methods" approach both, field-based studies in a real city, and lab-based studies with different city panoramas projected in a virtual environment will be conducted. The outcomes of this project will consist of 1) a platform for mobile outdoor gaze-based interaction, 2) a set of guidelines for explicit and 3) implicit gaze-based interaction methods for tourists visually exploring a city panorama.