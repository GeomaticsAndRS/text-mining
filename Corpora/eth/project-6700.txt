  Statistical Machine Learning with Aggregation Methods  

Bagging, Boosting and Functional Gradient Descent can be viewed as aggregation methods which average multiple predictions to form a new, often better final prediction. The research project focuses on theoretical understanding of aggregation methods and developing new, more efficient versions of aggregation for classification and nonparametric function estimation, particularly in the presence of very many predictor variables. The following problems will be addressed: (i) variance reduction of bagging for unstable procedures; (ii) consistency and asymptotic efficiency of boosting in classification and regression; (iii) alternative surrogate loss functions for boosting in classification; (iv) functional gradient descent for multivariate responses with general form of the nonparametric likelihood; (v) modifications of boosting for tumor classification with gene expression data.